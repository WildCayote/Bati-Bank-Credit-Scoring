{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the scripts to the notebooks path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\KifiyaAIM-Course\\Week - 6\\Bati-Bank-Credit-Scoring\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "print(current_dir)\n",
    "\n",
    "# Get the parent directory\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "scripts_path = os.path.join(parent_dir, 'scripts')\n",
    "\n",
    "# Insert the path to the parent directory\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "# Insert the path to the Scripts directory\n",
    "sys.path.insert(0, scripts_path)\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../data/data.csv\"\n",
    "\n",
    "data = pd.read_csv(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain RFMS scores and Use default estimator to add target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.credit_risk_modeler import CreditScoreEngine\n",
    "\n",
    "credit_engine = CreditScoreEngine(transaction_data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- obtain, score and label RFMS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfms_data = credit_engine.calcualte_rfms()\n",
    "scored_rfms = credit_engine.score_rfms(rfms_data=rfms_data)\n",
    "labeled_rfms, decision_boundary = credit_engine.label_rfms_score(data=scored_rfms, score_column='RFMS_Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the labeled rfms with the original data\n",
    "data = data.join(other=labeled_rfms, on='CustomerId', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.feature_engineering import FeatureEngineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Break down the `TransactionStartTime` into Hour, Day, Month and Year features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = FeatureEngineering.extract_date_features(data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = FeatureEngineering.handle_missing_data(data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Arggregate data per customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = FeatureEngineering.aggregate_customer_data(data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Normalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, scaler = FeatureEngineering.normalize_numerical_features(data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_encoding = {'Good': 1, 'Bad': 0}\n",
    "data['RiskLabel'] = data['RiskLabel'].map(lambda x: risk_encoding[x])\n",
    "data, encoders = FeatureEngineering.encode_categorical_data(data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Save the encoders and the numerical scalers to use when new data comes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "DUMP_PATH = '../data/'\n",
    "\n",
    "# serialize the scaler\n",
    "with open(os.path.join(DUMP_PATH, 'scaler.pkl'), 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n",
    "\n",
    "# serialize the encoder\n",
    "with open(os.path.join(DUMP_PATH, 'encoder.pkl'), 'wb') as file:\n",
    "    pickle.dump(scaler, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['RFMS_Score', 'RecencyScore', 'PricingStrategy', 'ProductCategory']\n",
    "target = 'RiskLabel'\n",
    "\n",
    "X = data[features]\n",
    "y = data[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume 'features' is your features DataFrame and 'target' is your binary target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and select models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Define the models in a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define hyperparameter seach spaces for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'Logistic Regression': {'C': [0.01, 0.1, 1, 10, 100]},\n",
    "    'Random Forest': {'n_estimators': [50, 100, 200], 'max_depth': [None, 5, 10]}, \n",
    "    'Decision Tree': {'max_depth': [3, 5, 7, None]},  \n",
    "    'Gradient Boosting': {'learning_rate': [0.01, 0.1, 0.2], 'n_estimators': [50, 100, 200]}  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Use the models and the parameter grids to define a greadsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_searches = {\n",
    "    name: GridSearchCV(estimator=model, param_grid=param_grids[name], cv=5, scoring='accuracy')\n",
    "    for name, model in models.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression model best parameters: {'C': 10}\n",
      "Random Forest model best parameters: {'max_depth': None, 'n_estimators': 50}\n",
      "Decision Tree model best parameters: {'max_depth': 3}\n",
      "Gradient Boosting model best parameters: {'learning_rate': 0.01, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "for name, grid_search in grid_searches.items():\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f\"{name} model best parameters: {grid_search.best_params_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
